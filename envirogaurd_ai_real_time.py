# -*- coding: utf-8 -*-
"""EnviroGaurd_AI_Real_time.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FIRniIv2CYqIvMWEnay3S3UjNwEt5She
"""

# EnviroGuard AI - Real-time Environmental Hazard Detection & Interactive Assistant
# Fixed Implementation

import os
import time
import cv2
import numpy as np
# The import for torch was unnecessary and has been removed.
from transformers import pipeline
# Installing the necessary packages using pip before importing them.
!pip install gradio
import gradio as gr
!pip install gTTS
from gtts import gTTS
!pip install transformers
!pip install langchain
!pip install speechrecognition
import speech_recognition as sr
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import threading
import queue
import datetime
import json

# Configuration
CONFIG = {
    "detection_interval": 1,  # seconds between detection runs
    "confidence_threshold": 0.6,  # minimum confidence for detection
    "save_detections": True,
    "detections_path": "detections/",
    "log_path": "logs/",
    "models": {
        # Using more reliable, publicly available models
        "image_classification": "google/vit-base-patch16-224",
        "object_detection": "facebook/detr-resnet-50",
        # Using a general model for fire/smoke detection
        "general_classification": "microsoft/resnet-50"
    },
    "alerts": {
        "fire": {"level": "critical", "message": "Fire detected! Evacuate immediately."},
        "smoke": {"level": "warning", "message": "Smoke detected. Investigate source."},
        "pollution": {"level": "warning", "message": "High pollution levels detected."},
        "wildlife": {"level": "info", "message": "Wildlife detected in monitored area."},
    },
    # Fire/smoke related keywords in general classification models
    "fire_keywords": ["fire", "flame", "burning", "blaze", "conflagration"],
    "smoke_keywords": ["smoke", "smog", "haze", "fog", "mist"],
    "pollution_keywords": ["smog", "haze", "pollution", "cloud", "dust"]
}


# Ensure necessary directories exist
for path in [CONFIG["detections_path"], CONFIG["log_path"]]:
    os.makedirs(path, exist_ok=True)

class EnviroGuardAI:
    def __init__(self):
        self.camera = None
        self.detection_queue = queue.Queue()
        self.alert_queue = queue.Queue()
        self.running = False
        self.detection_thread = None
        self.alert_thread = None
        self.last_frame = None
        self.history = []

        # Initialize models
        print("Loading models...")
        self.load_models()
        print("Models loaded successfully")

        # Initialize LangChain components
        self.setup_langchain()

        # Initialize speech components
        self.recognizer = sr.Recognizer()

    def load_models(self):
        """Load detection models"""
        try:
            # Image classification model
            self.classifier = pipeline("image-classification",
                                      model=CONFIG["models"]["image_classification"])

            # Object detection model
            self.object_detector = pipeline("object-detection",
                                          model=CONFIG["models"]["object_detection"])

            # General classification model for fire/smoke detection
            self.general_classifier = pipeline("image-classification",
                                             model=CONFIG["models"]["general_classification"])

            print("All models loaded successfully")
        except Exception as e:
            print(f"Error loading models: {e}")
            raise

    def setup_langchain(self):
        """Setup LangChain components for natural language interaction"""
        # Environmental assessment prompt template
        env_template = """
        You are EnviroGuard AI, an environmental monitoring assistant.
        Based on the following detections, please assess the situation and provide advice:

        Current Detections: {detections}
        Historical Context: {history}
        Current Time: {time}

        Question: {question}

        Provide a concise assessment and recommendation:
        """

        self.env_prompt = PromptTemplate(
            input_variables=["detections", "history", "time", "question"],
            template=env_template
        )

        # Initialize OpenAI LLM - Note: This requires an API key in actual implementation
        # For demonstration purposes, we'll comment out the actual initialization
        # self.llm = OpenAI(temperature=0.3)
        # self.chain = LLMChain(llm=self.llm, prompt=self.env_prompt)
        print("LangChain components initialized (Note: OpenAI LLM commented out)")

    def start_camera(self, camera_id=0):
        """Initialize and start the camera"""
        try:
            self.camera = cv2.VideoCapture(camera_id)
            if not self.camera.isOpened():
                raise Exception(f"Could not open camera {camera_id}")
            return True
        except Exception as e:
            print(f"Error starting camera: {e}")
            return False

    def stop_camera(self):
        """Stop and release the camera"""
        if self.camera:
            self.camera.release()
        self.camera = None

    def capture_frame(self):
        """Capture a frame from the camera"""
        if not self.camera:
            return None

        ret, frame = self.camera.read()
        if ret:
            self.last_frame = frame
            return frame
        return None

    def detect_hazards(self, frame):
        """Detect hazards in the given frame"""
        if frame is None:
            return []

        results = []

        # Convert frame for model input
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Fire and smoke detection using general classification
        general_results = self.general_classifier(rgb_frame)

        # Check for fire and smoke related classes
        for result in general_results:
            if result["score"] > CONFIG["confidence_threshold"]:
                # Check for fire-related labels
                if any(keyword in result["label"].lower() for keyword in CONFIG["fire_keywords"]):
                    results.append({"type": "fire", "confidence": result["score"]})

                # Check for smoke-related labels
                elif any(keyword in result["label"].lower() for keyword in CONFIG["smoke_keywords"]):
                    results.append({"type": "smoke", "confidence": result["score"]})

                # Check for pollution indicators
                elif any(keyword in result["label"].lower() for keyword in CONFIG["pollution_keywords"]):
                    results.append({
                        "type": "pollution",
                        "category": result["label"],
                        "confidence": result["score"]
                    })

        # Object detection for wildlife
        objects = self.object_detector(rgb_frame)
        for obj in objects:
            if obj["score"] > CONFIG["confidence_threshold"]:
                # Check if detected object is wildlife
                if obj["label"] in ["bird", "cat", "dog", "horse", "sheep", "cow",
                                   "elephant", "bear", "zebra", "giraffe", "deer"]:
                    results.append({
                        "type": "wildlife",
                        "species": obj["label"],
                        "confidence": obj["score"]
                    })

        # Additional classification to catch potential hazards
        classifications = self.classifier(rgb_frame)
        for classification in classifications:
            if classification["score"] > CONFIG["confidence_threshold"]:
                # Check for pollution indicators (if not already detected)
                if (classification["label"] in ["smog", "fog", "cloud", "haze"] and
                    not any(r["type"] == "pollution" for r in results)):
                    results.append({
                        "type": "pollution",
                        "category": classification["label"],
                        "confidence": classification["score"]
                    })

                # Additional fire detection
                if (any(keyword in classification["label"].lower() for keyword in CONFIG["fire_keywords"]) and
                    not any(r["type"] == "fire" for r in results)):
                    results.append({
                        "type": "fire",
                        "confidence": classification["score"]
                    })

        return results

    def process_alerts(self, detections):
        """Process detections and generate appropriate alerts"""
        alerts = []
        for detection in detections:
            detection_type = detection["type"]
            if detection_type in CONFIG["alerts"]:
                alert_config = CONFIG["alerts"][detection_type]
                alert = {
                    "level": alert_config["level"],
                    "message": alert_config["message"],
                    "detection": detection,
                    "timestamp": datetime.datetime.now().isoformat()
                }
                alerts.append(alert)

                # Add to alert queue for processing
                self.alert_queue.put(alert)

        return alerts

    def log_detection(self, detections, frame=None):
        """Log detection results and optionally save the frame"""
        if not detections:
            return

        # Create timestamp
        timestamp = datetime.datetime.now().isoformat().replace(":", "-")

        # Log to history
        self.history.append({
            "timestamp": timestamp,
            "detections": detections
        })

        # Trim history if too long
        if len(self.history) > 100:
            self.history = self.history[-100:]

        # Save detection data
        if CONFIG["save_detections"]:
            # Save detection data as JSON
            detection_file = os.path.join(CONFIG["log_path"], f"detection_{timestamp}.json")
            with open(detection_file, "w") as f:
                json.dump(detections, f)

            # Save frame if provided
            if frame is not None:
                frame_file = os.path.join(CONFIG["detections_path"], f"frame_{timestamp}.jpg")
                cv2.imwrite(frame_file, frame)

    def detection_loop(self):
        """Main detection loop running in a separate thread"""
        while self.running:
            frame = self.capture_frame()
            if frame is not None:
                # Run detection
                detections = self.detect_hazards(frame)

                # Process alerts
                alerts = self.process_alerts(detections)

                # Log detections
                if detections:
                    self.log_detection(detections, frame)

                # Put results in queue for UI
                self.detection_queue.put({
                    "frame": frame,
                    "detections": detections,
                    "alerts": alerts,
                    "timestamp": datetime.datetime.now().isoformat()
                })

            # Sleep for the configured interval
            time.sleep(CONFIG["detection_interval"])

    def alert_loop(self):
        """Process alerts in a separate thread"""
        while self.running:
            try:
                # Get alert from queue with timeout
                alert = self.alert_queue.get(timeout=1)

                # Generate speech for alert
                if alert["level"] in ["critical", "warning"]:
                    self.speak(alert["message"])

                self.alert_queue.task_done()
            except queue.Empty:
                pass

    def speak(self, text):
        """Convert text to speech and play it"""
        try:
            tts = gTTS(text=text, lang='en')
            audio_file = "alert.mp3"
            tts.save(audio_file)

            # Play the audio (platform-dependent)
            # This is a simple approach using os.system - in production, consider more robust solutions
            os.system(f"start {audio_file}" if os.name == "nt" else f"play {audio_file}")
        except Exception as e:
            print(f"Error generating speech: {e}")

    def listen(self):
        """Listen for voice commands"""
        try:
            with sr.Microphone() as source:
                print("Listening...")
                audio = self.recognizer.listen(source)
                text = self.recognizer.recognize_google(audio)
                print(f"Heard: {text}")
                return text
        except sr.UnknownValueError:
            print("Could not understand audio")
            return None
        except sr.RequestError as e:
            print(f"Could not request results; {e}")
            return None
        except Exception as e:
            print(f"Error listening: {e}")
            return None

    def answer_query(self, question, detections=None):
        """Use LangChain to answer environmental queries"""
        if detections is None:
            recent_detections = self.history[-5:] if self.history else []
        else:
            recent_detections = detections

        # In a real implementation, you would use the LLM chain
        # For demonstration, we'll provide sample responses

        # Simulated LLM response
        if "fire" in str(recent_detections).lower():
            return "DANGER: Fire detected in the monitored area. Please evacuate immediately and call emergency services."
        elif "smoke" in str(recent_detections).lower():
            return "Warning: Smoke detected. Please investigate the source and prepare for possible evacuation."
        elif "pollution" in str(recent_detections).lower():
            return "Air quality alert: Elevated pollution levels detected. Consider wearing a mask if going outside."
        elif "wildlife" in str(recent_detections).lower():
            return "Wildlife detected in the monitoring area. No immediate action required."
        else:
            return "Environment appears safe. No hazards detected in the monitoring area."

        # In production, uncomment this code
        """
        response = self.chain.run(
            detections=str(recent_detections),
            history=str(self.history[-20:]),
            time=datetime.datetime.now().isoformat(),
            question=question
        )
        return response
        """

    def start(self):
        """Start all detection and processing threads"""
        if self.running:
            return False

        # Start camera
        if not self.start_camera():
            return False

        self.running = True

        # Start detection thread
        self.detection_thread = threading.Thread(target=self.detection_loop)
        self.detection_thread.daemon = True
        self.detection_thread.start()

        # Start alert thread
        self.alert_thread = threading.Thread(target=self.alert_loop)
        self.alert_thread.daemon = True
        self.alert_thread.start()

        return True

    def stop(self):
        """Stop all threads and release resources"""
        self.running = False

        if self.detection_thread:
            self.detection_thread.join(timeout=2)

        if self.alert_thread:
            self.alert_thread.join(timeout=2)

        self.stop_camera()

    def get_current_frame_with_detections(self):
        """Get the current frame with detection annotations"""
        frame = self.last_frame
        if frame is None:
            return None

        # Get latest detections
        detections = []
        if not self.detection_queue.empty():
            try:
                result = self.detection_queue.get(block=False)
                detections = result.get("detections", [])
                # Put it back if needed by other functions
                self.detection_queue.put(result)
            except:
                pass

        # Draw detections on frame
        return draw_detection_results(frame, detections)

# Utility function to apply labels on the frame
def draw_detection_results(frame, detections):
    """Draw detection results on the frame"""
    result_frame = frame.copy()

    # Define colors for different detection types (BGR format)
    colors = {
        "fire": (0, 0, 255),      # Red
        "smoke": (128, 128, 128),  # Gray
        "pollution": (255, 0, 0),  # Blue
        "wildlife": (0, 255, 0)    # Green
    }

    # Draw detection labels
    y_offset = 30
    for detection in detections:
        detection_type = detection["type"]
        confidence = detection.get("confidence", 0)

        # Create label text
        label = f"{detection_type.upper()}: {confidence:.2f}"

        # Add species for wildlife
        if detection_type == "wildlife" and "species" in detection:
            label += f" ({detection['species']})"

        # Get color for this detection type
        color = colors.get(detection_type, (255, 255, 255))

        # Draw label
        cv2.putText(result_frame, label, (10, y_offset),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        y_offset += 25

    return result_frame

# Initialize the UI
def create_ui(enviroguard):
    """Create and launch the Gradio UI"""

    # Function to update the video feed - returns the frame for display
    def update_video():
        if enviroguard.last_frame is not None:
            # Draw detection results on the frame
            frame_with_detections = enviroguard.get_current_frame_with_detections()
            if frame_with_detections is not None:
                # Convert frame for display
                return cv2.cvtColor(frame_with_detections, cv2.COLOR_BGR2RGB)
        return None

    # Function to process text queries
    def process_query(query):
        if not query:
            return "Please enter a query."
        return enviroguard.answer_query(query)

    # Function to process voice input
    def voice_input():
        text = enviroguard.listen()
        if text:
            return text, enviroguard.answer_query(text)
        return "", "Could not understand audio"

    # Function to start monitoring
    def start_monitoring():
        if enviroguard.start():
            return "Monitoring started successfully"
        return "Failed to start monitoring"

    # Function to stop monitoring
    def stop_monitoring():
        enviroguard.stop()
        return "Monitoring stopped"

    # Function to get detection status
    def get_status():
        try:
            # Try to get the latest detection result without blocking
            if not enviroguard.detection_queue.empty():
                result = enviroguard.detection_queue.get(block=False)
                detections = result["detections"]
                alerts = result["alerts"]
                # Put it back for other functions to use
                enviroguard.detection_queue.put(result)

                # Build status message
                if not detections:
                    return "No hazards detected"

                status = "Detected: "
                for detection in detections:
                    status += f"{detection['type']} ({detection['confidence']:.2f}); "

                if alerts:
                    status += "\n\nALERTS:\n"
                    for alert in alerts:
                        status += f"[{alert['level'].upper()}] {alert['message']}\n"

                return status
            else:
                if not enviroguard.running:
                    return "Monitoring not active"
                return "Monitoring active, no new detections"
        except Exception as e:
            return f"Error getting status: {e}"

    # Create the interface
    with gr.Blocks(title="EnviroGuard AI") as interface:
        gr.Markdown("# EnviroGuard AI - Environmental Monitoring System")

        with gr.Row():
            with gr.Column(scale=2):
                # Use a standard Image component that gets updated with each refresh button click
                video_output = gr.Image(label="Camera Feed")
                refresh_video_button = gr.Button("Refresh Camera Feed")
                status_output = gr.Textbox(label="Detection Status", interactive=False)

            with gr.Column(scale=1):
                with gr.Row():
                    start_button = gr.Button("Start Monitoring")
                    stop_button = gr.Button("Stop Monitoring")

                gr.Markdown("## Environmental Assistant")
                text_input = gr.Textbox(label="Ask a question about the environment")
                voice_button = gr.Button("Voice Input")
                voice_text = gr.Textbox(label="Voice Recognition Result", interactive=False)
                response_output = gr.Textbox(label="Assistant Response", interactive=False)

                refresh_button = gr.Button("Refresh Status")

        # Set up event handlers
        start_button.click(start_monitoring, outputs=status_output)
        stop_button.click(stop_monitoring, outputs=status_output)
        text_input.submit(process_query, inputs=text_input, outputs=response_output)
        voice_button.click(voice_input, outputs=[voice_text, response_output])
        refresh_button.click(get_status, outputs=status_output)
        refresh_video_button.click(update_video, outputs=video_output)

    # Launch the interface
    interface.launch(share=True)

# Main function
def main():
    enviroguard = EnviroGuardAI()
    create_ui(enviroguard)

if __name__ == "__main__":
    main()

from tensorflow import keras

!pip install --upgrade langchain-core langchain-community langchain-openai
!pip install SpeechRecognition  # Note the capital S and R

# For OpenAI integration
from langchain_openai import OpenAI  # New recommended import

# For legacy compatibility (if needed)
from langchain_community.llms import OpenAI  # Alternative import

# The rest of your imports
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import speech_recognition as sr